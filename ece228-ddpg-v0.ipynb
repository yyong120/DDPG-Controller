{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97705483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T10:57:12.291657Z",
     "iopub.status.busy": "2024-06-05T10:57:12.290906Z",
     "iopub.status.idle": "2024-06-05T10:57:16.549283Z",
     "shell.execute_reply": "2024-06-05T10:57:16.548263Z"
    },
    "papermill": {
     "duration": 4.266079,
     "end_time": "2024-06-05T10:57:16.551931",
     "exception": false,
     "start_time": "2024-06-05T10:57:12.285852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "# from gekko import GEKKO\n",
    "from math import exp\n",
    "import sys\n",
    "import os\n",
    "from time import sleep, time\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "e = 2.71828182846\n",
    "\n",
    "T0 = 300.0\n",
    "V = 1.0\n",
    "k0 = 8.46e6\n",
    "Cp = 0.231\n",
    "sigma = 1000.0\n",
    "# Ts = 400.0\n",
    "Ts = 430.0\n",
    "Qs = 0.0\n",
    "F = 5.0\n",
    "E = 5e4\n",
    "delta_H = -1.15e4\n",
    "R = 8.314\n",
    "CAs = 2.0\n",
    "CA0s = 4.0\n",
    "\n",
    "P = np.array([[716.83, 0.0], [0.0, 1.0]])\n",
    "gamma = 9.53    # equation 45g in \"economic...\"\n",
    "x_size = 2\n",
    "u_size = 2\n",
    "c_hl_size = 8 # hidden layer for critic\n",
    "a_hl_size = 8 # hidden layer for actor\n",
    "u1_bd = 3.5\n",
    "# u2_bd = 5e5\n",
    "u2_bd = 5.0   # u2 is multified by 1e-5\n",
    "u2_scale = 1e5\n",
    "x1_bd = 1.0\n",
    "x2_bd = 26.0\n",
    "QMat = np.array([[3.0, 0.0], [0.0, 1.0]])\n",
    "RMat = np.array([[1.0/20.0, 0.0], [0.0, 1.0/30.0]])\n",
    "\n",
    "NUM_OF_X1 = 2\n",
    "NUM_OF_X2 = 3\n",
    "\n",
    "DISCOUNT = 0.99  # discount for q-learning rewards\n",
    "C_LR = 1e-2     # learning rate for critic\n",
    "A_LR = 1e-2     # learning rate for actor\n",
    "C_MAX_EPOCH = 50\n",
    "A_MAX_EPOCH = 50\n",
    "MAX_Q_STEP = 2\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "exception_num_0 = 0 # lyapunov constraints may conflict with u's bound\n",
    "exception_num_1 = 0 # even not considering lyapunov constraints, solution still not found\n",
    "\n",
    "'''\n",
    "x=0, u=0 -> dx/dt=0\n",
    "'''\n",
    "def getCA0s():\n",
    "    return V/F * k0 * np.exp(-E/(R*Ts)) * (CAs**2) + CAs\n",
    "\n",
    "def getQs():\n",
    "    return sigma * Cp * V * (F/V * (Ts-T0) + delta_H/(sigma*Cp) * k0 * np.exp(-E/(R*Ts)) * (CAs**2))\n",
    "\n",
    "CA0s = getCA0s()\n",
    "Qs = getQs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82037002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T10:57:16.560030Z",
     "iopub.status.busy": "2024-06-05T10:57:16.559447Z",
     "iopub.status.idle": "2024-06-05T10:57:16.609481Z",
     "shell.execute_reply": "2024-06-05T10:57:16.608189Z"
    },
    "papermill": {
     "duration": 0.057168,
     "end_time": "2024-06-05T10:57:16.612108",
     "exception": false,
     "start_time": "2024-06-05T10:57:16.554940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class GActionNoise():\n",
    "    def __init__(self, mu, sigma, decay_factor):\n",
    "        self.mu = np.array(mu)\n",
    "        self.sigma = np.array(sigma)\n",
    "        self.decay_factor = decay_factor\n",
    "    \n",
    "    def __call__(self):\n",
    "        x = np.random.normal(self.mu, self.sigma, self.mu.shape)\n",
    "        return x\n",
    "    \n",
    "    def decay(self):\n",
    "        self.sigma *= self.decay_factor\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_size, action_size, max_reward):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.max_reward = max_reward\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, input_size))\n",
    "        self.action_memory = np.zeros((self.mem_size, action_size))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_size))\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        idx = self.mem_cntr % self.mem_size\n",
    "        \n",
    "        self.state_memory[idx] = state\n",
    "        self.action_memory[idx] = action\n",
    "        self.reward_memory[idx] = reward\n",
    "        self.new_state_memory[idx] = new_state\n",
    "        self.terminal_memory[idx] = done\n",
    "        \n",
    "        self.mem_cntr += 1\n",
    "    \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        new_states = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, new_states, terminal\n",
    "\n",
    "\n",
    "class CriticFc(nn.Module):\n",
    "    def __init__(self, c_lr, input_size, fc1_dims, fc2_dims, action_size, \n",
    "                 input_bds, action_bds, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticFc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.action_size = action_size\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.input_bds = np.array(input_bds)\n",
    "        self.action_bds = np.array(action_bds)\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, name+'_ddpg.pkl')\n",
    "\n",
    "        # normalization layer\n",
    "        self.nl = nn.Linear(self.input_size + self.action_size, self.input_size + self.action_size)\n",
    "        state_action_bds = 1.0 / np.concatenate((self.input_bds, self.action_bds))\n",
    "        state_action_bds = np.diag(state_action_bds)\n",
    "        self.nl.weight = nn.Parameter(torch.tensor(state_action_bds))\n",
    "        self.nl.bias = nn.Parameter(torch.zeros(self.input_size + self.action_size))\n",
    "        for p in self.nl.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size + self.action_size, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.q = nn.Linear(self.fc2_dims, 1)\n",
    "        f3 = 0.01\n",
    "        nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
    "        nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
    "\n",
    "        self.optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=c_lr)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat((state, action), 1)\n",
    "        state_action = self.nl(state_action)\n",
    "        state_action_value = torch.relu(self.fc1(state_action))\n",
    "        state_action_value = torch.relu(self.fc2(state_action_value))\n",
    "        state_action_value = self.q(state_action_value)\n",
    "\n",
    "        return state_action_value\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        # print(\"... saving checkpoint ...\")\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        print(\"... loading checkpoint ...\")\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n",
    "\n",
    "\n",
    "class ActorFc(nn.Module):\n",
    "    def __init__(self, a_lr, input_size, fc1_dims, fc2_dims, action_size,\n",
    "                 input_bds, action_bds, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorFc, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.action_size = action_size\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.input_bds = np.array(input_bds)\n",
    "        self.action_bds = np.array(action_bds)\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, name + '_ddpg.pkl')\n",
    "\n",
    "        # normalization layer\n",
    "        self.nl = nn.Linear(self.input_size, self.input_size)\n",
    "        state_bds = np.diag(1.0 / self.input_bds)\n",
    "        self.nl.weight = nn.Parameter(torch.tensor(state_bds))\n",
    "        self.nl.bias = nn.Parameter(torch.zeros(self.input_size))\n",
    "        for p in self.nl.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.u = nn.Linear(self.fc2_dims, self.action_size)\n",
    "        f3 = 0.01\n",
    "        nn.init.uniform_(self.u.weight.data, -f3, f3)\n",
    "        nn.init.uniform_(self.u.bias.data, -f3, f3)\n",
    "\n",
    "        # scaling layer\n",
    "        self.sl = nn.Linear(self.action_size, self.action_size)\n",
    "        action_bds = np.diag(self.action_bds)\n",
    "        self.sl.weight = nn.Parameter(torch.tensor(action_bds))\n",
    "        self.sl.bias = nn.Parameter(torch.zeros(self.action_size))\n",
    "        for p in self.sl.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=a_lr)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = self.nl(state)\n",
    "        action = torch.relu(self.fc1(state))\n",
    "        action = torch.relu(self.fc2(action))\n",
    "        action = torch.tanh(self.u(action))\n",
    "        action = self.sl(action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # print(\"... saving checkpoint ...\")\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        print(\"... loading checkpoint ...\")\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n",
    "\n",
    "\n",
    "class AgentFc():\n",
    "    def __init__(self, a_lr, c_lr, input_size, action_size, input_bds, action_bds, max_reward,\n",
    "                 action_noise_mu, action_noise_sigma, action_noise_decay, tau,\n",
    "                 env, discount=0.99, max_size=10000, layer1_size=12,\n",
    "                 layer2_size=8, batch_size=64, chkpt_dir='tmp/ddpg'):\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, input_size, action_size, max_reward)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.critic = CriticFc(c_lr, input_size, layer1_size, layer2_size,\n",
    "                               action_size, input_bds, action_bds, 'Critic', chkpt_dir).double()\n",
    "        self.actor = ActorFc(a_lr, input_size, layer1_size, layer2_size,\n",
    "                             action_size, input_bds, action_bds, 'Actor', chkpt_dir).double()\n",
    "        self.target_critic = CriticFc(c_lr, input_size, layer1_size, layer2_size,\n",
    "                                      action_size, input_bds, action_bds, 'TargetCritic', chkpt_dir).double()\n",
    "        self.target_actor = ActorFc(a_lr, input_size, layer1_size, layer2_size,\n",
    "                                    action_size, input_bds, action_bds, 'TargetActor', chkpt_dir).double()\n",
    "        \n",
    "        self.noise = GActionNoise(action_noise_mu, action_noise_sigma, action_noise_decay)\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "    \n",
    "    def choose_action(self, observation, add_noise):\n",
    "        observation = torch.tensor(observation).double().to(self.actor.device)\n",
    "        u = self.actor(observation).to(self.actor.device)\n",
    "        if add_noise:\n",
    "            u += torch.tensor(self.noise()).double().to(self.actor.device)\n",
    "        \n",
    "        return u.cpu().detach().numpy()\n",
    "    \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def get_target_critic_value(self, reward, new_critic_value_tensor, done):\n",
    "        '''\n",
    "        'new_critic_value_tensor' is a tensor containing a single element,\n",
    "        so use new_critic_value_tensor.item() to get its value\n",
    "\n",
    "        if done == 0:\n",
    "            not a terminal state\n",
    "        if done == 1:\n",
    "            exceed the range\n",
    "        if done == 2:\n",
    "            converge to 0\n",
    "        '''\n",
    "        ### argmax ###\n",
    "        # if done == 0:\n",
    "        #     return reward + self.discount * new_critic_value_tensor.item()\n",
    "        # if done == 1:\n",
    "        #     return reward + 0.0\n",
    "        # return reward + self.memory.max_reward\n",
    "\n",
    "        ### argmin ###\n",
    "        if done == 0:\n",
    "            return reward + self.discount * new_critic_value_tensor.item()\n",
    "        if done == 2:\n",
    "            return reward + 0.0\n",
    "        return reward + self.memory.max_reward\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        state = torch.tensor(state).double().to(self.critic.device)\n",
    "        action = torch.tensor(action).double().to(self.critic.device)\n",
    "        reward = torch.tensor(reward).double().to(self.critic.device)\n",
    "        new_state = torch.tensor(new_state).double().to(self.critic.device)\n",
    "\n",
    "        new_action = self.target_actor(new_state)\n",
    "        new_critic_value = self.target_critic(new_state, new_action)\n",
    "        critic_value = self.critic(state, action)\n",
    "\n",
    "        target_critic_value = list(map(self.get_target_critic_value, reward, new_critic_value, done))\n",
    "        target_critic_value = torch.tensor(target_critic_value).to(self.critic.device)\n",
    "        target_critic_value = target_critic_value.view(self.batch_size, 1)\n",
    "\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss()\n",
    "        critic_loss = loss(target_critic_value, critic_value)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        u = self.actor(state)\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        # actor_loss = -self.critic(state, u) # argmax\n",
    "        actor_loss = self.critic(state, u)  # argmin\n",
    "        actor_loss = torch.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "        \n",
    "        self.update_network_parameters()\n",
    "    \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_dict = dict(critic_params)\n",
    "        actor_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "\n",
    "        for name in critic_dict:\n",
    "            critic_dict[name] = tau * critic_dict[name].clone() + \\\n",
    "                                (1-tau) * target_critic_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_dict)\n",
    "        \n",
    "        for name in actor_dict:\n",
    "            actor_dict[name] = tau * actor_dict[name].clone() + \\\n",
    "                                (1-tau) * target_actor_dict[name].clone()\n",
    "        \n",
    "        self.target_actor.load_state_dict(actor_dict)\n",
    "    \n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "    \n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7319f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T10:57:16.619421Z",
     "iopub.status.busy": "2024-06-05T10:57:16.619042Z",
     "iopub.status.idle": "2024-06-05T10:57:16.636936Z",
     "shell.execute_reply": "2024-06-05T10:57:16.635700Z"
    },
    "papermill": {
     "duration": 0.024429,
     "end_time": "2024-06-05T10:57:16.639371",
     "exception": false,
     "start_time": "2024-06-05T10:57:16.614942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CstrEnv():\n",
    "    def __init__(self, obs_flag, he=1e-4, hs=0.01, x0=np.zeros(2)):\n",
    "        self.x = x0.copy()\n",
    "        self.he = he\n",
    "        self.hs = hs\n",
    "        self.obs_flag = obs_flag\n",
    "    \n",
    "    def get_reward(self, xk, uk):\n",
    "        # reward_x = 1.0 / (np.fabs(xk[0]) + 1.0) + 1.0 / (np.fabs(xk[1]) + 1.0)\n",
    "        # # if self.obs_flag == '1':\n",
    "        # #     reward_x = 1.0 / (np.fabs(xk[0]) + 1.0)\n",
    "        # # elif self.obs_falg == '2':\n",
    "        # #     reward_x = 1.0 / (np.fabs(xk[1]) + 1.0)\n",
    "\n",
    "        # reward_u = 0.01 / (np.fabs(uk[0]) + 1.0) + 0.01 / (np.fabs(uk[1]) + 1.0)\n",
    "\n",
    "        # return reward_x + reward_u\n",
    "\n",
    "        reward_x = 3 * np.sqrt(np.fabs(xk[0])) + np.sqrt(np.fabs(xk[1]))\n",
    "        reward_u = (uk**2 / np.array([20.0, 30.0])).sum()\n",
    "\n",
    "        return reward_x + reward_u\n",
    "\n",
    "    \n",
    "    def get_new_state(self, xk, uk, add_noise=False, noise=None):\n",
    "        xk1 = xk.copy()\n",
    "        if not add_noise:\n",
    "            for _ in range(int(self.hs / self.he)):\n",
    "                xk1[0] += self.he * (F/V * (uk[0] + CA0s - xk1[0] - CAs) - k0 * np.exp(-E/R/(xk1[1] + Ts)) * (xk1[0] + CAs)**2)\n",
    "                xk1[1] += self.he * (F/V * (T0 - xk1[1] - Ts) + (-delta_H)/sigma/Cp * k0 * np.exp(-E/R/(xk1[1] + Ts)) * (xk1[0] + CAs)**2 + (uk[1] * u2_scale  + Qs)/sigma/Cp/V)\n",
    "        else:\n",
    "            for _ in range(int(self.hs / self.he)):\n",
    "                xk1[0] += self.he * (F/V * (uk[0] + CA0s - xk1[0] - CAs) - k0 * np.exp(-E/R/(xk1[1] + Ts)) * (xk1[0] + CAs)**2) + self.he * noise[0]\n",
    "                xk1[1] += self.he * (F/V * (T0 - xk1[1] - Ts) + (-delta_H)/sigma/Cp * k0 * np.exp(-E/R/(xk1[1] + Ts)) * (xk1[0] + CAs)**2 + (uk[1] * u2_scale  + Qs)/sigma/Cp/V) + self.he * noise[1]\n",
    "\n",
    "        return xk1\n",
    "    \n",
    "    def reset(self, x0=None):\n",
    "        if x0 is None:\n",
    "            self.x = np.random.uniform([-x1_bd, -x2_bd], [x1_bd, x2_bd], size=(2,))\n",
    "        else:\n",
    "            self.x = x0.copy()\n",
    "        \n",
    "        if self.obs_flag == '1':\n",
    "            return np.array([self.x[0]])\n",
    "        elif self.obs_flag == '2':\n",
    "            return np.array([self.x[1]])\n",
    "        return self.x\n",
    "    \n",
    "    def step(self, uk, add_noise=False, noise=None):\n",
    "        reward = self.get_reward(self.x, uk)\n",
    "        new_state = self.get_new_state(self.x, uk, add_noise, noise)\n",
    "        done = 0\n",
    "        if abs(new_state[0]) > x1_bd or abs(new_state[1]) > x2_bd:\n",
    "            done = 1\n",
    "        elif abs(new_state[0]) < 0.001 and abs(new_state[1]) < 0.001:\n",
    "            done = 2\n",
    "        \n",
    "        self.x = new_state.copy()\n",
    "\n",
    "        if self.obs_flag == '1':\n",
    "            new_state = np.array([new_state[0]])\n",
    "        elif self.obs_flag == '2':\n",
    "            new_state = np.array([new_state[1]])\n",
    "        \n",
    "        return new_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff3fa90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T10:57:16.646678Z",
     "iopub.status.busy": "2024-06-05T10:57:16.646250Z",
     "iopub.status.idle": "2024-06-05T10:57:18.171810Z",
     "shell.execute_reply": "2024-06-05T10:57:18.170577Z"
    },
    "papermill": {
     "duration": 1.532022,
     "end_time": "2024-06-05T10:57:18.174291",
     "exception": false,
     "start_time": "2024-06-05T10:57:16.642269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = CstrEnv('12', x0=np.random.uniform([-x1_bd, -x2_bd], [x1_bd, x2_bd], size=(2,)))\n",
    "agent = AgentFc(a_lr=1e-4, c_lr=1e-3, input_size=2, action_size=2,\n",
    "                input_bds=[x1_bd, x2_bd], action_bds=[u1_bd, u2_bd], max_reward=500,\n",
    "                action_noise_mu=[0.0, 0.0], action_noise_sigma=[0.3, 0.005],\n",
    "                action_noise_decay=0.5, tau=0.005, env=env, discount=0.98,\n",
    "                max_size=100000, layer1_size=400, layer2_size=300,\n",
    "                batch_size=64, chkpt_dir='/kaggle/working')\n",
    "\n",
    "# agent.load_models()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "rewards = []\n",
    "steps = []\n",
    "\n",
    "rewards_file = '/kaggle/working/rewards.npy'\n",
    "steps_file = '/kaggle/working/steps.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e920dacb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T10:57:18.181938Z",
     "iopub.status.busy": "2024-06-05T10:57:18.181414Z",
     "iopub.status.idle": "2024-06-05T16:40:35.452298Z",
     "shell.execute_reply": "2024-06-05T16:40:35.451093Z"
    },
    "papermill": {
     "duration": 20597.569454,
     "end_time": "2024-06-05T16:40:35.746748",
     "exception": false,
     "start_time": "2024-06-05T10:57:18.177294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [5:43:17<00:00,  4.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# agent.load_models()\n",
    "\n",
    "\n",
    "for episode in tqdm(range(1, 5001)):\n",
    "    obs = env.reset()\n",
    "    done = 0\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done and step < 200:\n",
    "        step += 1\n",
    "        act = agent.choose_action(obs, add_noise=True)\n",
    "        new_state, reward, done = env.step(act)\n",
    "        agent.remember(obs, act, reward, new_state, done)\n",
    "        agent.learn()\n",
    "        obs = new_state\n",
    "        \n",
    "        total_reward += reward\n",
    "    \n",
    "    # print(done)\n",
    "    if done == 1:\n",
    "        total_reward += 500\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    steps.append(step)\n",
    "    \n",
    "    if episode % 200 == 0:\n",
    "        agent.save_models()\n",
    "        np.save(rewards_file, np.array(rewards))\n",
    "        np.save(steps_file, np.array(steps))\n",
    "    \n",
    "    # if episode % 1000 == 0:\n",
    "    #     agent.noise.decay()\n",
    "\n",
    "agent.save_models()\n",
    "\n",
    "rewards = np.array(rewards)\n",
    "steps = np.array(steps)\n",
    "np.save(rewards_file, rewards)\n",
    "np.save(steps_file, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f5f5d0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T16:40:36.359569Z",
     "iopub.status.busy": "2024-06-05T16:40:36.359186Z",
     "iopub.status.idle": "2024-06-05T16:40:36.364099Z",
     "shell.execute_reply": "2024-06-05T16:40:36.362836Z"
    },
    "papermill": {
     "duration": 0.314783,
     "end_time": "2024-06-05T16:40:36.366267",
     "exception": false,
     "start_time": "2024-06-05T16:40:36.051484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# r = np.load('/kaggle/working/rewards.npy')\n",
    "# s = np.load('/kaggle/working/steps.npy')\n",
    "# print(len(r), len(s))\n",
    "# plt.figure()\n",
    "# plt.plot(range(len(r)), r)\n",
    "# plt.figure()\n",
    "# plt.plot(range(len(s)), s)\n",
    "# plt.figure()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20608.520588,
   "end_time": "2024-06-05T16:40:37.830545",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-05T10:57:09.309957",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
